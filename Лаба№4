1. Ошибка теста: ложно-положительный результат (False Positive, FP).
   Результат теста оказался ложноположительным: он ошибочно указал на наличие заболевания у человека, который на самом деле здоров.

2.  Метрики классификации для матрицы ошибок TP = 5, TN = 90, FP = 10, FN = 5:
   1. Точность (Accuracy): (TP + TN) / (TP + TN + FP + FN) = (5 + 90) / 100 = 0.95
   2. Чувствительность (Sensitivity, Recall): TP / (TP + FN) = 5 / (5 + 5) = 0.5
   3. Специфичность (Specificity): TN / (TN + FP) = 90 / (90 + 10) = 0.9
   4. Полнота (Precision): TP / (TP + FP) = 5 / (5 + 10) = 0.33
   5. F1-мера: 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.33 * 0.5) / (0.33 + 0.5) ≈ 0.38

3. Сравнение классификаторов:
1. Первый классификатор очень хорошо находит все случаи заболевания (высокая чувствительность), но при этом часто ошибочно относит здоровых людей к больным (низкая специфичность). Это приводит к большому количеству ложных тревог.
2. Второй классификатор демонстрирует более сбалансированные результаты как по чувствительности, так и по специфичности. Это говорит о его большей надежности в определении как больных, так и здоровых людей.
Разница в производительности классификаторов может объясняться особенностями данных, на которых они обучались. Вероятно, данные для каждого классификатора имеют различную структуру и распределение, что влияет на их способность различать классы.

4. Гиперпараметры модели логистической регрессии:
   1. Регуляризация (L1, L2, параметры λ или C): Помогает бороться с переобучением модели, контролируя сложность модели.
   2.Метод оптимизации и его параметры (например, скорость обучения в градиентном спуске): Определяет, как модель будет подбирать свои параметры, чтобы минимизировать ошибку.
   3.Число итераций для обучения модели: Указывает, сколько раз модель будет "просматривать" данные для обучения.
   4.Функция потерь: Определяет, как измеряется ошибка модели, что влияет на направление корректировки параметров.

5. Если нужно определить, какой из числовых признаков оказывает наибольшее влияние на целевую переменную (например, 'Price(euro)'), то линейная регрессия из библиотеки scikit-learn - будет самым удобным инструментом для этого. 
Нужно применить следующие действия:
 1. Подготовка данных:
Загружаем данные в структуру DataFrame, которая удобна для обработки и анализа.
Извлекаем только числовые признаки, так как линейная регрессия работает с числовыми данными.
 2.Разделение данных:
 -Делим данные на две части: обучающую выборку (для обучения модели) и тестовую выборку (для оценки качества модели на новых данных).
 3.Обучение модели:
 -Используем обучающую выборку, чтобы обучить модель линейной регрессии. Модель "учится" на данных, находя закономерности между признаками и целевой переменной.
 4.Анализ важности признаков:
 -Анализируем веса (коэффициенты) модели, чтобы понять, какие признаки оказывают наибольшее влияние на целевую переменную. Признаки с наибольшими весами считаются наиболее важными.

Пример кода:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import numpy as np
# Шаг 1: Загрузка данных в DataFrame (предположим, что данные в файле 'cars.csv')
df = pd.read_csv('cars.csv')
# Шаг 2: Выбор только числовых признаков и целевой переменной
numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()
numerical_features.remove('Price(euro)')  # Удаляем целевую переменную из списка признаков
X = df[numerical_features]
y = df['Price(euro)']
# Шаг 3: Разделение данных на обучающие и тестовые наборы
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Шаг 4: Обучение модели линейной регрессии
model = LinearRegression()
model.fit(X_train, y_train)
# Шаг 5: Анализ весов модели
coefficients = pd.Series(model.coef_, index=numerical_features)
coefficients = coefficients.sort_values(ascending=False)
print("Веса модели линейной регрессии:")
print(coefficients)
# Параметр, который в наибольшей степени связан с целевой переменной
most_related_feature = coefficients.idxmax()
print(f'Параметр, который в наибольшей степени связан с ценой: {most_related_feature}')

Описание шагов:
 1. Инструменты:
 -Импортируем библиотеки pandas для работы с данными, scikit-learn для машинного обучения и numpy для операций с числами.
 2.Данные:
 -Загружаем данные о машинах из файла 'cars.csv' в таблицу DataFrame.
 3.Подготовка признаков:
 -Выбираем только столбцы с числовыми признаками, исключая столбец 'Price(euro)', который будет нашей целевой переменной.
 4.Разделение на выборки:
 -Делим данные на две части: обучающую и тестовую выборки, используя функцию train_test_split. Это нужно, чтобы обучить модель на одной части данных и проверить ее качество на другой, не виденной ранее части.
 5.Обучение модели:
 -Создаем модель линейной регрессии и обучаем ее на обучающей выборке, чтобы она нашла зависимость между числовыми признаками и ценой автомобиля.
 6.Анализ весов:
 -Извлекаем веса (коэффициенты) модели, которые хранятся в атрибуте coef_.
 -Сортируем веса по убыванию, чтобы увидеть, какой признак оказывает наибольшее влияние на цену автомобиля.
  
 6. Значение функции сигмоиды σ(z) для z = 0.25:
   σ(0.25) = 1 / (1 + e^(-0.25)) ≈ 0.5596

Для вычисления значения функции сигмоиды σ(z) при z = 0.25 на Python, мы можем использовать библиотеку math для доступа к функции экспоненты.

Функция сигмоиды определяется формулой:

[ \sigma(z) = \frac{1}{1 + e^{-z}} ]

где ( e ) — основание натурального логарифма.

import math

def sigmoid(z):
    """Вычисление значения сигмоидной функции"""
    return 1 / (1 + math.exp(-z))

# Значение z
z = 0.25

# Вычисление сигмоиды для z
sigmoid_value = sigmoid(z)

print(f"Значение функции сигмоиды σ({z}) = {sigmoid_value:.4f}")
В данном коде имеется:
 1. Функция sigmoid принимает в качестве аргумента значение ( z ) и возвращает значение сигмоидной функции для этого ( z ).
 2. Мы используем math.exp для вычисления ( e^{-z} ).
 3. Форматирование вывода .4f используется для ограничения вывода до четырёх знаков после запятой, что позволяет нам получить приблизительное значение 0.5596.

 7. Значение производной функции сигмоиды σ'(z) для z = -3:
   σ'(-3) = σ(-3) * (1 - σ(-3)) = (1 / (1 + e^3)) * (e^3 / (1 + e^3)) ≈ 0.0474
Для вычисления значения производной функции сигмоиды ( \sigma'(z) ) при ( z = -3 ) в Python, мы снова воспользуемся библиотекой math.
Производная функции сигмоиды по ( z ) выражается через саму функцию сигмоиды следующим образом:

[ \sigma'(z) = \sigma(z) \cdot (1 - \sigma(z)) ]

Уравнение производной сигмоидной функции можно выразить через саму сигмоидную функцию. Это свойство делает сигмоиду очень удобной для использования, так как упрощает и ускоряет вычисления, необходимые, например, при обучении нейронных сетей.

import math

def sigmoid(z):
    """Вычисление значения сигмоидной функции"""
    return 1 / (1 + math.exp(-z))

def sigmoid_derivative(z):
    """Вычисление значения производной сигмоидной функции"""
    sig = sigmoid(z)
    return sig * (1 - sig)

# Значение z
z = -3

# Вычисление производной сигмоиды для z
sigmoid_derivative_value = sigmoid_derivative(z)

print(f"Значение производной функции сигмоиды σ'({z}) = {sigmoid_derivative_value:.4f}")

 В данном примере:
1. Функция sigmoid определяет значение сигмоидной функции.
2. Функция sigmoid_derivative применяет значение, полученное от sigmoid(z), для расчета производной сигмоиды, используя формулу ( \sigma'(z) = \sigma(z) \cdot (1 - \sigma(z)) ).
3. Переменная ( z = -3 ) используется для вычисления производной.
Результат отображается с помощью функции print, при этом вывод форматируется до четырех десятичных знаков для повышения точности представления.

8.  Классификация по логистической модели для z = 0.1 с порогом 0.6:
   Поскольку σ(0.1) ≈ 0.5244, что меньше порога 0.6, результат классификации будет отнесен к отрицательному классу.
Для выполнения классификации по логистической модели на Python, для начала мы выявим функцию сигмоиды, которая будет вычислять вероятность принадлежности к положительному классу.
Далее мы создадим функцию для классификации, которая принимает значение ( z ) и порог (threshold), и на основании сравнения значения сигмоиды с порогом определяет класс объекта.

import math

def sigmoid(z):
    """Вычисление значения сигмоидной функции"""
    return 1 / (1 + math.exp(-z))

def classify(z, threshold=0.6):
    """Классификация с использованием порога для сигмоидной функции"""
    probability = sigmoid(z)
    if probability >= threshold:
        return 'положительный класс'
    else:
        return 'отрицательный класс'

# Значение z
z = 0.1

# Классификация для заданного z с порогом 0.6
classification_result = classify(z)

print(f"Классификация для z = {z} с порогом 0.6: {classification_result}")

Пояснение кода:

 1. Функция sigmoid(z): Определяет вероятность того, что данное значение ( z ) относится к положительному классу.
 2. Функция classify(z, threshold=0.6): Получает значение ( z ) и устанавливает пороговое значение (по умолчанию 0.6). Сравнивает результат сигмоиды с этим порогом и возвращает строку, указывающую на классификацию объекта.
 3. Использование функции classify(z): Передаём значение ( z = 0.1 ) и получаем ответ о классификации. Если результат сигмоиды равен или превышает 0.6, объект считается принадлежащим к положительному классу; в противном случае — к отрицательному.

9. Значение бинарной кросс-энтропии для предсказания y' = 0.1 и целевой переменной y = 1, где y' - это предсказание модели y:*
   L(y, y') = -y * log(y') - (1 - y) * log(1 - y') = -1 * log(0.1) - (1 - 1) * log(0.9) ≈ 2.3026

Решение на Python:

import numpy as np
from sklearn.metrics import log_loss
# Заданные значения
y_true = 1
y_pred = 0.1
# Вычисление вручную
bce_manual = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
print(f'Бинарная кросс-энтропия (вычисление вручную): {bce_manual}')

# Проверка с использованием scikit-learn
# log_loss принимает массивы, поэтому преобразуем значения в массивы
bce_sklearn = log_loss([y_true], [y_pred], labels=[0, 1])
print(f'Бинарная кросс-энтропия (scikit-learn): {bce_sklearn}')

Пояснение кода:
 1. Подключаем нужные библиотеки: numpy для выполнения математических вычислений и log_loss из sklearn.metrics для расчета логарифмической потери.
 2. Определяем фактическое значение y_true и прогнозируемое значение y_pred.
 3. Рассчитываем бинарную кросс-энтропию вручную с применением соответствующей формулы.
 4. Для подтверждения корректности результата применяем функцию log_loss из библиотеки scikit-learn.

Запуск кода должен вывести:
  Бинарная кросс-энтропия (вычисление вручную): 2.3025850929940455
  Бинарная кросс-энтропия (scikit-learn): 2.3025850929940455
Оба подхода должны привести к идентичному результату, что свидетельствует о верности проведенных расчетов.
